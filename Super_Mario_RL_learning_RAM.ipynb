{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install and import the game and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the super mario game in the notebook\n",
    "import gym_super_mario_bros\n",
    "\n",
    "#Import the Joypad wrapper in the notebook\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "#Import the simple controls so that |the model just needs to control some movements of our agent (here Mario)\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes the game from colour image (RGB) to grayscale so that our processing becomes faster as we need to deal with less data \n",
    "from gym import ObservationWrapper\n",
    "from gym.spaces import Box\n",
    "\n",
    "# VecFrameStack allows us to work with our stacked enviroments by letting us know the information of previous frames. DummyVecEnv transforms our model so that we can pass it to our AI model. \n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecMonitor\n",
    "\n",
    "# Import the Super Mario RAM utils\n",
    "from Super_Mario_RAM_utils import MarioRAMGrid\n",
    "\n",
    "# Import Numpy for mathematics\n",
    "import numpy as np\n",
    "\n",
    "# Import pyplot for plotting\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import time for measuring the training time\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the optimization frame - HPO\n",
    "# import optuna\n",
    "# Bring in the eval policy method for metric calculation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gc\n",
    "# Import os for file path management\n",
    "import os\n",
    "\n",
    "# Import PPO algorithm to train our model\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Import Base Callback for saving models and to continue from there\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMAndSkipWrapper(ObservationWrapper):\n",
    "    def __init__(self, env, n_stack=4, n_skip=2):\n",
    "        super().__init__(env)\n",
    "        self.n_stack = n_stack\n",
    "        self.n_skip = n_skip\n",
    "        self.width = 16\n",
    "        self.height = 13\n",
    "        self.observation_space = Box(\n",
    "            low=-2, high=2, shape=(self.height, self.width, self.n_stack), dtype=np.int8\n",
    "        )\n",
    "        \n",
    "        self.frame_stack = np.zeros((self.height, self.width, (self.n_stack-1)*self.n_skip+1), dtype=np.int8)\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        grid = MarioRAMGrid(self.env)\n",
    "        frame = grid.rendered_screen # The RAM map for the current frame\n",
    "        \n",
    "        self.frame_stack[:,:,1:] = self.frame_stack[:,:,:-1] # Shift frame_stack by 1 to the right\n",
    "        self.frame_stack[:,:,0] = frame # Add the current frame to stack on the left\n",
    "        obs = self.frame_stack[:,:,::self.n_skip]\n",
    "        return obs\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        self.frame_stack = np.zeros((self.height, self.width, (self.n_stack-1)*self.n_skip+1), dtype=np.int8)\n",
    "        grid = MarioRAMGrid(self.env)\n",
    "        frame = grid.rendered_screen # 2d array\n",
    "\n",
    "        for i in range(self.frame_stack.shape[-1]):\n",
    "            self.frame_stack[:,:,i] = frame\n",
    "\n",
    "        obs = self.frame_stack[:,:,::self.n_skip]\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(seed, env_name, n_stack, n_skip):\n",
    "    def init():\n",
    "        env = gym_super_mario_bros.make(env_name)\n",
    "        env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "        env = RAMAndSkipWrapper(env, n_stack=n_stack, n_skip=n_skip)\n",
    "\n",
    "        return env\n",
    "    \n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of trained and logged files \n",
    "CHECKPOINT_DIR = './train/WithSkipWrapper'\n",
    "LOG_DIR = './logs'\n",
    "HPO_LOG_DIR = './opt_logs'\n",
    "HPO_CHECKPOINT_DIR = './opt_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"SuperMarioBros-1-2-v0\"\n",
    "n_stack = 4\n",
    "n_skip = 4\n",
    "\n",
    "env = make_env(0, env_name, n_stack, n_skip)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 16, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test env_wrap\n",
    "done = True\n",
    "for i in range(150):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, n_stack, figsize=(14,10))\n",
    "for i in range(n_stack):\n",
    "    ax[i].imshow(state[:,:,n_stack-i-1], vmin=-2, vmax=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SubprocVecEnv([make_env(i, env_name, n_stack, n_skip) for i in range(6)])\n",
    "env = VecMonitor(env, LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_ewc = DummyVecEnv([make_env(0, \"SuperMarioBros-1-1-v0\", n_stack, n_skip)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setup RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the learning rate\n",
    "def linear_schedule(initial_value: float):\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the trainnig files and logging files location\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Save the model and track training progress\n",
    "        if self.num_timesteps % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.num_timesteps))\n",
    "            self.model.save(model_path, exclude=['ewc', 'env_ewc'])\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = PPO.load(os.path.join(CHECKPOINT_DIR, 'best_model_2000040'), env, tensorboard_log=LOG_DIR)\n",
    "#model = PPO('MlpPolicy', env, tensorboard_log='./logs/WithRAMWrapper/linear_learning_rate', verbose=0, learning_rate=linear_schedule(3e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./train/WithRAMWrapper/model_1/world_1_stage_1/best_model_4700094', env=env, env_ewc=env_ewc, custom_objects={'learning_rate': linear_schedule(1e-4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    }
   ],
   "source": [
    "model._instantiate_ewc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (13x64 and 832x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mfeatures_extractor(batch_obs)\n\u001b[0;32m      5\u001b[0m features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mfeatures_extractor(batch_obs)\n\u001b[1;32m----> 6\u001b[0m actor_data \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m critic_data \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_critic(features)\n\u001b[0;32m      8\u001b[0m actor_logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39maction_net(actor_data)\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:233\u001b[0m, in \u001b[0;36mMlpExtractor.forward_actor\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (13x64 and 832x64)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_obs = torch.tensor(state, dtype=torch.float32, device='cuda')\n",
    "features = model.policy.features_extractor(batch_obs)\n",
    "features = model.policy.features_extractor(batch_obs)\n",
    "actor_data = model.policy.mlp_extractor.forward_actor(features)\n",
    "critic_data = model.policy.mlp_extractor.forward_critic(features)\n",
    "actor_logits = model.policy.action_net(actor_data)\n",
    "value_estimates = model.policy.value_net(critic_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000, -1.0000,  0.9997, -0.8733, -0.9989, -1.0000,  0.4860,  1.0000,\n",
       "         -0.9999, -0.9999, -0.9921, -0.9142, -0.5380,  1.0000, -1.0000, -1.0000,\n",
       "          1.0000, -0.9789, -0.9983,  0.9999,  1.0000, -0.8674,  0.9987, -1.0000,\n",
       "         -0.9999, -0.9992,  0.9890,  0.9999, -1.0000, -0.8687, -0.9998,  0.9992,\n",
       "          0.9952, -0.9993, -1.0000, -1.0000,  1.0000,  1.0000, -0.9953, -1.0000,\n",
       "          1.0000,  0.9570,  1.0000,  0.9882,  1.0000, -0.9998,  1.0000, -1.0000,\n",
       "          0.9998,  0.9996,  0.9999,  0.9999, -1.0000,  1.0000,  0.9973,  0.1640,\n",
       "         -0.9981, -0.9996,  0.9997,  0.9999, -1.0000,  1.0000, -1.0000,  1.0000]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=832, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=832, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=7, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensorboard_log = './logs/WithRAMWrapper/linear_learning_rate/model_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of trained and logged files \n",
    "CHECKPOINT_DIR = './train/WithRAMWrapper/model_1/world_1_stage_2_try_8'\n",
    "LOG_DIR = './logs'\n",
    "HPO_LOG_DIR = './opt_logs'\n",
    "HPO_CHECKPOINT_DIR = './opt_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=100002, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2578dbfae20>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=5e6, callback=callback, tb_log_name=\"model_1_nivel_2_try_8\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "super_mario_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
