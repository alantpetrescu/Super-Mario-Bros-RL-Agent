{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install and import the game and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the super mario game in the notebook\n",
    "import gym_super_mario_bros\n",
    "\n",
    "#Import the Joypad wrapper in the notebook\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "#Import the simple controls so that the model just needs to control some movements of our agent (here Mario)\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes the game from colour image (RGB) to grayscale so that our processing becomes faster as we need to deal with less data \n",
    "from gym import ObservationWrapper\n",
    "from gym.spaces import Box\n",
    "\n",
    "# VecFrameStack allows us to work with our stacked enviroments by letting us know the information of previous frames. DummyVecEnv transforms our model so that we can pass it to our AI model. \n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecMonitor\n",
    "\n",
    "# Import the Super Mario RAM utils\n",
    "from Super_Mario_RAM_utils import MarioRAMGrid\n",
    "\n",
    "# Import Numpy for mathematics\n",
    "import numpy as np\n",
    "\n",
    "# Import pyplot for plotting\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import time for measuring the training time\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the optimization frame - HPO\n",
    "# import optuna\n",
    "# Bring in the eval policy method for metric calculation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gc\n",
    "# Import os for file path management\n",
    "import os\n",
    "\n",
    "# Import PPO algorithm to train our model\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Import Base Callback for saving models and to continue from there\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMAndSkipWrapper(ObservationWrapper):\n",
    "    def __init__(self, env, n_stack=4, n_skip=2):\n",
    "        super().__init__(env)\n",
    "        self.n_stack = n_stack\n",
    "        self.n_skip = n_skip\n",
    "        self.width = 16\n",
    "        self.height = 13\n",
    "        self.observation_space = Box(\n",
    "            low=-2, high=2, shape=(self.height, self.width, self.n_stack), dtype=np.int8\n",
    "        )\n",
    "        \n",
    "        self.frame_stack = np.zeros((self.height, self.width, (self.n_stack-1)*self.n_skip+1), dtype=np.int8)\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        grid = MarioRAMGrid(self.env)\n",
    "        frame = grid.rendered_screen # The RAM map for the current frame\n",
    "        \n",
    "        self.frame_stack[:,:,1:] = self.frame_stack[:,:,:-1] # Shift frame_stack by 1 to the right\n",
    "        self.frame_stack[:,:,0] = frame # Add the current frame to stack on the left\n",
    "        obs = self.frame_stack[:,:,::self.n_skip]\n",
    "        return obs\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        self.frame_stack = np.zeros((self.height, self.width, (self.n_stack-1)*self.n_skip+1), dtype=np.int8)\n",
    "        grid = MarioRAMGrid(self.env)\n",
    "        frame = grid.rendered_screen # 2d array\n",
    "\n",
    "        for i in range(self.frame_stack.shape[-1]):\n",
    "            self.frame_stack[:,:,i] = frame\n",
    "\n",
    "        obs = self.frame_stack[:,:,::self.n_skip]\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(seed, env_name, n_stack, n_skip):\n",
    "    def init():\n",
    "        env = gym_super_mario_bros.make(env_name)\n",
    "        env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "        env = RAMAndSkipWrapper(env, n_stack=n_stack, n_skip=n_skip)\n",
    "\n",
    "        return env\n",
    "    \n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of trained and logged files \n",
    "CHECKPOINT_DIR = './train/WithSkipWrapper'\n",
    "LOG_DIR = './logs'\n",
    "HPO_LOG_DIR = './opt_logs'\n",
    "HPO_CHECKPOINT_DIR = './opt_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"SuperMarioBros-1-2-v0\"\n",
    "n_stack = 4\n",
    "n_skip = 4\n",
    "\n",
    "env = make_env(0, env_name, n_stack, n_skip)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 16, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test env_wrap\n",
    "done = True\n",
    "for i in range(150):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, n_stack, figsize=(14,10))\n",
    "for i in range(n_stack):\n",
    "    ax[i].imshow(state[:,:,n_stack-i-1], vmin=-2, vmax=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SubprocVecEnv([make_env(i, env_name, n_stack, n_skip) for i in range(6)])\n",
    "env = VecMonitor(env, LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_ewc = DummyVecEnv([make_env(0, \"SuperMarioBros-1-1-v0\", n_stack, n_skip)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setup RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the learning rate\n",
    "def linear_schedule(initial_value: float):\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the trainnig files and logging files location\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Save the model and track training progress\n",
    "        if self.num_timesteps % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.num_timesteps))\n",
    "            self.model.save(model_path, exclude=['ewc', 'env_ewc'])\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = PPO.load(os.path.join(CHECKPOINT_DIR, 'best_model_2000040'), env, tensorboard_log=LOG_DIR)\n",
    "#model = PPO('MlpPolicy', env, tensorboard_log='./logs/WithRAMWrapper/linear_learning_rate', verbose=0, learning_rate=linear_schedule(3e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./train/WithRAMWrapper/model_1/world_1_stage_1/best_model_4700094', env=env, env_ewc=env_ewc, custom_objects={'learning_rate': linear_schedule(2e-4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env_ewc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    }
   ],
   "source": [
    "model._instantiate_ewc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_obs = torch.tensor(state, dtype=torch.float32, device='cuda')\n",
    "features = model.policy.features_extractor(batch_obs)\n",
    "features = model.policy.features_extractor(batch_obs)\n",
    "actor_data = model.policy.mlp_extractor.forward_actor(features)\n",
    "critic_data = model.policy.mlp_extractor.forward_critic(features)\n",
    "actor_logits = model.policy.action_net(actor_data)\n",
    "value_estimates = model.policy.value_net(critic_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9996, -1.0000, -0.4641, -0.9938,  1.0000, -1.0000,  0.8914,  0.9990,\n",
       "         -0.9829, -1.0000, -0.9472, -0.5598,  0.0319,  1.0000, -1.0000, -1.0000,\n",
       "          0.9887,  1.0000, -0.9999, -0.9153,  1.0000, -0.9651, -0.7749, -1.0000,\n",
       "         -0.9998, -1.0000, -0.7749,  0.9395, -1.0000,  0.9998,  0.3940,  0.7203,\n",
       "         -0.9986, -0.9473, -1.0000, -0.9988,  1.0000,  1.0000, -0.9611, -1.0000,\n",
       "         -0.9980, -0.9968,  0.9464, -0.9998,  1.0000,  0.9999,  0.9852, -1.0000,\n",
       "         -1.0000, -1.0000, -0.9278,  1.0000, -1.0000, -0.9007, -0.7828, -1.0000,\n",
       "          1.0000, -1.0000, -0.9995, -0.9729, -1.0000,  0.8833, -1.0000,  0.9996]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=832, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=832, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=7, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensorboard_log = './logs/WithRAMWrapper/linear_learning_rate/model_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensorboard_log = './logs/WithRAMWrapper/linear_learning_rate/model_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of trained and logged files \n",
    "CHECKPOINT_DIR = './train/WithRAMWrapper/model_1/world_1_stage_2_try_11'\n",
    "LOG_DIR = './logs'\n",
    "HPO_LOG_DIR = './opt_logs'\n",
    "HPO_CHECKPOINT_DIR = './opt_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=100002, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_1_nivel_2_try_11\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:350\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    339\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m     reset_num_timesteps: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:247\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    243\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 247\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:166\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 166\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\stable_baselines3\\common\\policies.py:589\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[0;32m    588\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs)\n\u001b[1;32m--> 589\u001b[0m latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;66;03m# Evaluate the values for the given observations\u001b[39;00m\n\u001b[0;32m    591\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:230\u001b[0m, in \u001b[0;36mMlpExtractor.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m:return: latent_policy, latent_value of the specified network.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    If all layers are shared, then ``latent_policy == latent_value``\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    229\u001b[0m shared_latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared_net(features)\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net(shared_latent), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_latent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\super_mario_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1e7, callback=callback, tb_log_name=\"model_1_nivel_2_try_11\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "super_mario_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
