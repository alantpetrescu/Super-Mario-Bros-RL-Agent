{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the super mario game in the notebook\n",
    "import gym_super_mario_bros\n",
    "\n",
    "#Import the Joypad wrapper in the notebook\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "#Import the simple controls so that the model just needs to control some movements of our agent (here Mario)\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes the game from colour image (RGB) to grayscale so that our processing becomes faster as we need to deal with less data \n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation   \n",
    "\n",
    "#VecFrameStack allows us to work with our stacked enviroments by letting us know the information of previous frames. DummyVecEnv transforms our model so that we can pass it to our AI model. \n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SkipWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_stack=4, n_skip=2):\n",
    "        super().__init__(env)\n",
    "        self.n_stack = n_stack\n",
    "        self.n_skip = n_skip\n",
    "        self.height = 84\n",
    "        self.width = 84\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=255, shape=(self.height, self.width, self.n_stack), dtype=np.uint8\n",
    "        )\n",
    "        self.frame_stack = np.zeros((self.height, self.width, (self.n_stack-1)*self.n_skip+1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        self.frame_stack[:,:,1:] = self.frame_stack[:,:,:-1] # shift frame_stack by 1\n",
    "        self.frame_stack[:,:,0] = obs[:,:,0] # add current frame to stack\n",
    "        obs = self.frame_stack[:,:,::self.n_skip]\n",
    "        return obs\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        obs = obs[:,:,0]\n",
    "        self.frame_stack = np.zeros((self.height, self.width, (self.n_stack-1)*self.n_skip+1), dtype=np.uint8)\n",
    "        for i in range(self.frame_stack.shape[-1]):\n",
    "            self.frame_stack[:,:,i] = obs\n",
    "        obs = self.frame_stack[:,:,::self.n_skip]\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "env = ResizeObservation(env, (84, 84))\n",
    "# env = SkipWrapper(env, n_stack=4, n_skip=4)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(env, clip_obs=255, clip_reward=15)\n",
    "env = VecFrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# env = SkipWrapper(env, n_stack=4, n_skip=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 84, 84, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 84, 84, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, rewards, dones, infos = env.step([env.action_space.sample()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "n_stack=4\n",
    "\n",
    "fig, ax = plt.subplots(1, n_stack, figsize=(14,10))\n",
    "for i in range(n_stack):\n",
    "    obs = states[0, :,:,i]\n",
    "    obs = cv2.cvtColor(obs, cv2.COLOR_BGR2RGB)\n",
    "    ax[i].imshow(obs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(seed, env_name):\n",
    "    def init():\n",
    "        env = gym_super_mario_bros.make(env_name)\n",
    "        env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "        env = GrayScaleObservation(env, keep_dim=True)\n",
    "        env = ResizeObservation(env, (84, 84))\n",
    "        # env = SkipWrapper(env, n_stack=4, n_skip=4)\n",
    "        env.seed(seed)\n",
    "\n",
    "        return env\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DummyVecEnv([make_env(0, \"SuperMarioBros-1-1-v0\")])\n",
    "env = VecNormalize(env, clip_obs=255, clip_reward=15)\n",
    "env = VecFrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for file path management\n",
    "import os\n",
    "\n",
    "# Import PPO algorithm to train our model\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Import Base Callback for saving models and to continue from there\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of trained and logged files \n",
    "CHECKPOINT_DIR = './train/WithoutSkipWrapper/constant_learning'\n",
    "LOG_DIR = './logs'\n",
    "HPO_LOG_DIR = './opt_logs'\n",
    "HPO_CHECKPOINT_DIR = './opt_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(os.path.join(CHECKPOINT_DIR, 'model_2/best_model_5000100'), env)\n",
    "# model = PPO.load(os.path.join(HPO_CHECKPOINT_DIR, 'trial_1_best_model'), env, tensorboard_log=LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=1, render=True, deterministic=True)\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=1, render=True, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.610414672642946"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.reset()\n",
    "\n",
    "#Loop through the game\n",
    "while True:\n",
    "    # we are getting two values of which we need only one, so we put a underscore to just assign it the extra value\n",
    "    actions, _ = model.predict(states, deterministic=False)\n",
    "    states, rewards, dones, infos = env.step(actions)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m images\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[0;32m     13\u001b[0m actions, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(states, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 14\u001b[0m states, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m done \u001b[38;5;241m=\u001b[39m dones[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     16\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_frame_stack.py:48\u001b[0m, in \u001b[0;36mVecFrameStack.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]], np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray, List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],]:\n\u001b[1;32m---> 48\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     observations, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackedobs\u001b[38;5;241m.\u001b[39mupdate(observations, dones, infos)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observations, rewards, dones, infos\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_normalize.py:149\u001b[0m, in \u001b[0;36mVecNormalize.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m    143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    Apply sequence of actions to sequence of environments\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    actions -> (observations, rewards, dones)\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m    where ``dones`` is a boolean vector indicating whether each element is new.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_obs \u001b[38;5;241m=\u001b[39m obs\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_reward \u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py:76\u001b[0m, in \u001b[0;36mVecMonitor.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m---> 76\u001b[0m     obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 43\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\gym\\core.py:324\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    323\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m--> 324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m, reward, done, info\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\gym\\wrappers\\resize_observation.py:23\u001b[0m, in \u001b[0;36mResizeObservation.observation\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobservation\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINTER_AREA\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observation\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     27\u001b[0m         observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(observation, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "images = []\n",
    "states = env.reset()\n",
    "img = env.render(mode=\"rgb_array\")\n",
    "\n",
    "for i in range(10):\n",
    "    done = False\n",
    "    while done == False:\n",
    "        img = np.copy(env.render(mode=\"rgb_array\"))\n",
    "        images.append(img)\n",
    "        actions, _ = model.predict(states, deterministic=False)\n",
    "        states, rewards, dones, infos = env.step(actions)\n",
    "        done = dones[0]\n",
    "        env.render()\n",
    "\n",
    "gif = [img for i, img in enumerate(images) if i % 2 == 0]\n",
    "imageio.mimsave(\"super_mario_bros_run.gif\", gif, fps=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "super_mario_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
