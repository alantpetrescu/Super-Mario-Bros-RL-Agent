model1 -> world 1 stage 2 -> try 1 {
    Am schimbat EWC_utils ca sa calculeze gradientul pe un mini_batch de 64 in loc sa calculeze gradientul
    pentru fiecare observatie in parte si sa ia in calcul si reteaua de critic, nu numai actorul
    Applied EWC with n_samples 2048 and importance 0.5 and lr = linear_schedule(1e-4) for 5M operations
    Learning rate-ul too low or too much importance on the first task
}

model1 -> world 1 stage 2 -> try 2 {
    Applied EWC with n_samples 2048 and importance 0.4 and lr = linear_schedule(1e-4) for 5M operations
    Da, tot prost invata. Ii creste loss-ul la fel ca la celelalte modele
    Modelul e instabil. Poate sa ii scad din nou importanta si sa folosesc un alt tip de learning scheduler
}

model1 -> world 1 stage 2 -> try 3 {
    Applied EWC with n_samples 2048 and importance 0.5 and lr = linear_schedule(5e-5) for 5M operations
    Da, tot prost invata. Ii creste loss-ul la fel ca la celelalte modele
    Modelul e instabil. Poate sa ii scad din nou importanta si sa folosesc un alt tip de learning scheduler
}