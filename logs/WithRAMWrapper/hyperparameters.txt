model1 -> world 1 stage 1 { -> n_stack=4, n_skip=4 -> Perfect! Nota 11/10 -> A invatat doar dupa 4.8M de operatii cum sa joace bine
    learning_rate: linear_schedule(3e-4),
    n_steps: 2048,
    batch_size: 64,
    n_epochs: 10,
    gamma: 0.99,
    gae_lambda: 0.95,
    clip_range: 0.2,
    normalize_advantage: True,
    ent_coef: 0,
    vf_coef: 0.5,
    max_grad_norm: 0.5,
}

model1 -> world 1 stage 2 -> try 1 {
    Am schimbat EWC_utils ca sa calculeze gradientul pe un mini_batch de 64 in loc sa calculeze gradientul
    pentru fiecare observatie in parte si sa ia in calcul si reteaua de critic, nu numai actorul
    Applied EWC with n_samples 2048 and importance 0.5 and lr = linear_schedule(1e-4) for 5M operations
    Learning rate-ul too low or too much importance on the first task
}

model1 -> world 1 stage 2 -> try 2 {
    Applied EWC with n_samples 2048 and importance 0.4 and lr = linear_schedule(1e-4) for 5M operations
    Da, tot prost invata. Ii creste loss-ul la fel ca la celelalte modele
    Modelul e instabil. Poate sa ii scad din nou importanta si sa folosesc un alt tip de learning scheduler
}

model1 -> world 1 stage 2 -> try 3 {
    Applied EWC with n_samples 2048 and importance 0.5 and lr = linear_schedule(5e-5) for 5M operations
    Da, tot prost invata. Ii creste loss-ul la fel ca la celelalte modele
    Modelul e instabil. Poate sa ii scad din nou importanta si sa folosesc un alt tip de learning scheduler
}

model1 -> world 1 stage 2 -> try 4 {
    Applied EWC with n_samples 2048 and importance 0.6 and lr = linear_schedule(5e-5) for 5M operations
    Da, tot prost invata. Ii creste loss-ul la fel ca la celelalte modele
    Modelul e instabil. Poate sa ii scad din nou importanta si sa folosesc un alt tip de learning scheduler
}